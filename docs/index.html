<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GeoGen</title>
    <link rel="shortcut icon" type="image/jpg" href="img/favicon.ico" />
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>

</head>

<body>
    <nav class="navbar is-dark" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                    <img src="img/Microsoft-logo.svg" alt="Mesh Labs – Cambridge" style="height: 1.4rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                        Mesh Labs
                    </a>
                </div>
                <div class="navbar-end">
                    <a class="navbar-item" href="https://cvpr.thecvf.com/">
                        <img class="is-hidden-touch" src="img/cvpr-logo-light.png" style="height: 100%;" alt="CVPR 2024">
                        <img class="is-hidden-desktop" src="img/cvpr-logo-dark.png" style="height: 100%;" alt="CVPR 2024">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-2 is-size-3-mobile is-spaced has-text-centered">
                GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions
            </h1>
            <p class="subtitle is-5 has-text-centered has-text-grey">
                Computer Vision <br class="is-hidden-tablet"> and Pattern Recognition 2024
            </p>
            <p class="subtitle is-6 has-text-centered authors mt-5" style="line-height: 1.5;">

                <span>
                    <a href="https://iamsalvatore.io/">Salvatore&nbsp;Esposito</a>
                </span>
                <span>
                    <a href="https://ghixu.github.io/">Qingshan&nbsp;Xu</a>
                </span>
                <span>
                    <a href="https://kacper.ai/">Kacper&nbsp;Kania</a>
                </span>
                <span>
                    <a href="https://chewitt.me/">Charlie&nbsp;Hewitt</a>
                </span>
                <span>
                    <a href="mailto:omariott@exseed.ed.ac.uk">Octave&nbsp;Mariotti</a>
                </span>
                <span>
                    <a href="https://lohit.dev/">Lohit&nbsp;Petikam</a>
                </span>
                <span>
                    <a href="https://scholar.google.co.uk/citations?user=pZPD0hMAAAAJ&hl=en">Julien&nbsp;Valentin</a>
                </span>
                <span>
                    <a href="https://homepages.inf.ed.ac.uk/aonken/">Arno&nbsp;Onken</a>
                </span>
                <span>
                    <a href="https://homepages.inf.ed.ac.uk/omacaod/">Oisin&nbsp;Mac&nbsp;Aodha</a>
                </span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <a href="https://openaccess.thecvf.com/content/CVPR2024W/GCV/html/Esposito_GeoGen_Geometry-Aware_Generative_Modeling_via_Signed_Distance_Functions_CVPRW_2024_paper.html" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/2406.04254" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <a href="https://github.com/microsoft/GeoGen" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Dataset</span>
            </a>
        </div>
    </section>
    <section>
        <div class="container is-max-desktop">
            <figure class="image">
                <img src="img/sx-data.jpg" class="is-hidden-mobile"/>
                <img src="img/sx-data-narrow.jpg" class="is-hidden-tablet" />
            </figure>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                We introduce a new generative approach for synthesizing 3D geometry and images from single-view collections. 
                Most existing approaches predict volumetric density to render multi-view consistent images. By employing volumetric rendering using neural radiance fields, they inherit a key limi- tation: the generated geometry is noisy and unconstrained, limiting the quality and utility of the output meshes. 
                To address this issue, we propose GeoGen, a new SDF-based 3D generative model trained in an end-to-end manner. Initially, we reinterpret the volumetric density as a Signed Distance Function (SDF). This allows us to introduce useful priors to generate valid meshes. 
                However, those priors prevent the generative model from learning details, limiting the applicability of the method to real-world scenarios. To alle- viate that problem, we make the transformation learnable and constrain the rendered depth map to be consistent with the zero-level set of the SDF. 
                Through the lens of adversarial training, we encourage the network to produce higher fidelity details on the output meshes. 
                For evaluation, we in- troduce a synthetic dataset of human avatars captured from 360-degree camera angles, to overcome the challenges pre- sented by real-world datasets, which often lack 3D consistency and do not cover all camera angles. 
                Our experiments on multiple datasets show that GeoGen produces visually and quantitatively better geometry than the previous generative models based on neural radiance fields.
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Motivation
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    A common pitfall in the pursuit of higher image quality is the sidelining of the quality of the underlying geometry. This issue is evident in datasets like FFHQ, which lack accurate camera parameters and ground truth meshes, relying instead on approximate camera parameters. Such limitations hinder their applicability for tasks requiring precise geometric information. 
                    Previous approaches, such as EG3D and StyleSDF, have attempted to address these issues with varying degrees of success. State-of-the-art face recognition models are trained on millions of real human face images collected from the internet. 
                    <b>GeoGen</b> aims to tackle three major problems associated with such large-scale face recognition datasets.
                </p>
                <p>
                    The FFHQ dataset's lack of accurate camera parameters and ground truth meshes limits its use for precise geometric tasks. This motivated us to create and release a new dataset with accurate camera parameters, and 7 images of 512×512 for each of 10,000 identities, enhancing realism and usability in 3D applications.
                </p>
                <ul>
                    <li>
                        <b>Ethical issues</b> -
                        Many existing datasets are obtained by collecting web images without explicit consent.
                        Our digital faces are created using a generative model built from high quality head scans of a small number of individuals <i>obtained with consent</i>.
                    </li>
                    <li>
                        <b>Labeling noise and Camera parameters Inaccuracies</b> -
                        Web images collected by searching the names of celebrities often contain errors.
                        Our synthetic data has <i>guaranteed correctness of camera parameetrs and labels</i>.
                    </li>
                </ul>
            </div>
        </div>
    </section>    
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                About the Dataset
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    The GeoGen dataset is a comprehensive collection of over 70,000 synthetic face images designed for advanced face recognition and 3D geometry reconstruction research.
                </p>
                <p>
                    The dataset is essential for training deep learning models that are geared towards high-fidelity 3D facial geometry reconstruction.
                </p>
                <p>
                    This dataset was introduced in our paper titled GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions.
                </p>
                <p>
                    The dataset contains:
                </p>
                <ul>
                    <li>
                        The dataset comprises 70,000 images distributed across 10,000 identities, with each identity represented by 7 unique images. These images are captured from various camera angles spanning a full 360-degree view to enhance the diversity and comprehensiveness of the dataset, in addition to their camera parameters.
                    </li>
                </ul>
                <p>
                    We build on the synthetic face generation framework of <a href="https://microsoft.github.io/FaceSynthetics/">Wood et al.</a> to create a dataset of <b>over one million</b> synthetic face images.
                    This synthetics dataset helps us to overcome three primary shortcomings of existing large-scale face recognition datasets:
                </p>
                <p>
                    We define identity as a unique combination of facial geometry, texture, eye color, and hair style.
                    For each identity, we sample a set of accessories including clothing, make-up, glasses, face-wear, and head-wear.
                </p>
            </div>
            <div class="columns">
                <div class="column">
                    <img src="img/accessories_id1.png" />
                </div>
                <div class="column is-hidden-mobile">
                    <img src="img/accessories_id2.png" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    After sampling the identity and the accessories, we can render multiple images by varying the pose, expression, environment (lighting and background) and camera.
                </p>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
             Quantitative 3D Reconstruction Results
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    Our latest advances in 3D geometry reconstruction, as detailed in our findings for both ShapeNet Cars and Synthetic Heads, demonstrate significant improvements over previous methods. By incorporating Signed Distance Functions (SDF) and Depth Loss, GeoGen achieves superior accuracy and detail in reconstructed models.
                </p>
                <div class="table-container">
                    <table class="table">
                        <caption>Comparison of different 3D reconstruction metrics for generative models on <em>ShapeNet Cars</em> and our <em>Synthetic Heads</em> dataset. We report averages for MSE, HD, and MSD metrics. Variations of GeoGen without the SDF and Depth Loss constraints are also shown. Best methods for each dataset are bolded.</caption>
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>Chamfer ↓</th>
                                <th>MSE ↓</th>
                                <th>HD ↓</th>
                                <th>EMD ↓</th>
                                <th>MSD ↓</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <th colspan="6">ShapeNet Cars</th>
                            </tr>
                            <tr>
                                <td>EG3D</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>0.85</td>
                                <td>0.44</td>
                                <td>0.33</td>
                            </tr>
                            <tr>
                                <td>GeoGen w/o SDF&Depth Loss</td>
                                <td>0.27</td>
                                <td>0.28</td>
                                <td><b>0.77</b></td>
                                <td>0.42</td>
                                <td>0.31</td>
                            </tr>
                            <tr>
                                <td><b>GeoGen</b></td>
                                <td><b>0.25</b></td>
                                <td><b>0.27</b></td>
                                <td><b>0.77</b></td>
                                <td><b>0.40</b></td>
                                <td><b>0.29</b></td>
                            </tr>
                            <tr>
                                <th colspan="6">Synthetic Heads</th>
                            </tr>
                            <tr>
                                <td>EG3D</td>
                                <td>0.21</td>
                                <td>0.29</td>
                                <td>0.65</td>
                                <td>0.54</td>
                                <td>0.35</td>
                            </tr>
                            <tr>
                                <td>GeoGen w/o SDF& Depth Loss</td>
                                <td>0.19</td>
                                <td>0.29</td>
                                <td>0.59</td>
                                <td>0.45</td>
                                <td>0.26</td>
                            </tr>
                            <tr>
                                <td><b>GeoGen</b></td>
                                <td><b>0.17</b></td>
                                <td><b>0.27</b></td>
                                <td><b>0.56</b></td>
                                <td><b>0.43</b></td>
                                <td><b>0.24</b></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <p>
                    These results highlight our model's capability in providing detailed and accurate reconstructions, reducing metrics like Chamfer and MSE significantly across all tested models, and improving handling metrics like HD, EMD, and MSD.
                </p>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    The precision in our 3D models showcases our capability to tackle complex reconstruction challenges. These results are pivotal for applications requiring precise geometric data and serve as a benchmark for future developments in the field.
                </p>
            </div>
        </div>
    </section>    
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
                @inproceedings{esposito2024geogen,
                    title={GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions},
                    author={Esposito, Salvatore and Xu, Qingshan and Kania, Kacper and Hewitt, Charlie and Mariotti, Octave and Petikam, Lohit and Valentin, Julien and Onken, Arno and Mac Aodha, Oisin},
                    booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)},
                    year={2024}
                    organization={IEEE}
                  }
</pre>
        </div>
    </section>
    <footer class="footer pb-0">
        <div class="content has-text-centered pb-5">
            <p>
                Work conducted at <a
                    href=https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge>Mesh Labs &ndash;
                    Cambridge</a>.<br />
                <img src="img/Microsoft-logo-only.svg" class="mt-5" alt="Microsoft" style="height: 2rem;">
            </p>
        </div>
        <div class="footer-links content has-text-centered pt-5 has-text-grey-lighter is-size-7">
            <a href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy</a>
            <a href="https://go.microsoft.com/fwlink/?LinkID=206977">Terms of Use</a>
            <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks">Trademarks</a>
            <a href="https://microsoft.com">&copy; Microsoft 2024</a>
        </div>
    </footer>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>
