<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GeoGen</title>
    <link rel="shortcut icon" type="image/jpg" href="img/favicon.ico" />
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>

</head>

<body>
    <nav class="navbar is-dark" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                    <img src="img/Microsoft-logo.svg" alt="Mesh Labs – Cambridge" style="height: 1.4rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                        Mesh Labs
                    </a>
                </div>
                <div class="navbar-end">
                    <a class="navbar-item" href="https://wacv2023.thecvf.com/">
                        <img class="is-hidden-touch" src="img/wacv-logo-light.png" style="height: 100%;" alt="WACV 2023">
                        <img class="is-hidden-desktop" src="img/wacv-logo-dark.png" style="height: 100%;" alt="WACV 2023">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-2 is-size-3-mobile is-spaced has-text-centered">
                GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions
            </h1>
            <p class="subtitle is-5 has-text-centered has-text-grey">
                Computer Vision <br class="is-hidden-tablet"> and Pattern Recognition 2024
            </p>
            <p class="subtitle is-6 has-text-centered authors mt-5" style="line-height: 1.5;">

                <span>
                    <a href="https://iamsalvatore.io/">Salvatore&nbsp;Esposito</a>
                </span>
                <span>
                    <a href="https://ghixu.github.io/">Qingshan&nbsp;Xu</a>
                </span>
                <span>
                    <a href="https://kacper.ai/">Kacper&nbsp;Kania</a>
                </span>
                <span>
                    <a href="https://chewitt.me/">Charlie&nbsp;Hewitt</a>
                </span>
                <span>
                    <a href="mailto:omariott@exseed.ed.ac.uk">Octave&nbsp;Mariotti</a>
                </span>
                <span>
                    <a href="mailto:lohitpetikam@microsoft.com">Lohit&nbsp;Petikam</a>
                </span>
                <span>
                    <a href="mailto:juvalen@microsoft.com">Julien&nbsp;Valentin</a>
                </span>
                <span>
                    <a href="mailto:aonken@inf.ed.ac.uk">Arno&nbsp;Onken</a>
                </span>
                <span>
                    <a href="mailto:oisin.macaodha@ed.ac.uk">Oisin&nbsp;MacAodha</a>
                </span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <a href="https://openaccess.thecvf.com/content/WACV2023/html/Bae_DigiFace-1M_1_Million_Digital_Face_Images_for_Face_Recognition_WACV_2023_paper.html" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/2210.02579" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <a href="https://github.com/microsoft/GeoGen" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Dataset</span>
            </a>
        </div>
    </section>
    <section>
        <div class="container is-max-desktop">
            <figure class="image">
                <img src="img/sx-data.jpg" class="is-hidden-mobile"/>
                <img src="img/sx-data-narrow.jpg" class="is-hidden-tablet" />
            </figure>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                We introduce a new generative approach for synthesizing 3D geometry and images from single-view collections. 
                Most existing approaches predict volumetric density to render multi-view consistent images. By employing volumetric rendering using neural radiance fields, they inherit a key limi- tation: the generated geometry is noisy and unconstrained, limiting the quality and utility of the output meshes. 
                To address this issue, we propose GeoGen, a new SDF-based 3D generative model trained in an end-to-end manner. Initially, we reinterpret the volumetric density as a Signed Distance Function (SDF). This allows us to introduce useful priors to generate valid meshes. 
                However, those priors prevent the generative model from learning details, limiting the applicability of the method to real-world scenarios. To alle- viate that problem, we make the transformation learnable and constrain the rendered depth map to be consistent with the zero-level set of the SDF. 
                Through the lens of adversarial training, we encourage the network to produce higher fidelity details on the output meshes. 
                For evaluation, we in- troduce a synthetic dataset of human avatars captured from 360-degree camera angles, to overcome the challenges pre- sented by real-world datasets, which often lack 3D consistency and do not cover all camera angles. 
                Our experiments on multiple datasets show that GeoGen produces visually and quantitatively better geometry than the previous generative models based on neural radiance fields.
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Motivation
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    A common pitfall in the pursuit of higher image quality is the sidelining of the quality of the underlying geometry. This issue is evident in datasets like FFHQ, which lack accurate camera parameters and ground truth meshes, relying instead on approximate camera parameters. Such limitations hinder their applicability for tasks requiring precise geometric information. 
                    Previous approaches, such as EG3D and StyleSDF, have attempted to address these issues with varying degrees of success. State-of-the-art face recognition models are trained on millions of real human face images collected from the internet. 
                    <b>GeoGen</b> aims to tackle three major problems associated with such large-scale face recognition datasets.
                </p>
                <p>
                    The FFHQ dataset's lack of accurate camera parameters and ground truth meshes limits its use for precise geometric tasks. This motivated us to create and release a new dataset with accurate camera parameters, ground truth meshes, and 10 images of 512×512 for each of 19,800 identities, enhancing realism and usability in 3D applications.
                </p>
                <ul>
                    <li>
                        <b>Ethical issues</b> -
                        Many existing datasets are obtained by collecting web images without explicit consent.
                        Our digital faces are created using a generative model built from high quality head scans of a small number of individuals <i>obtained with consent</i>.
                    </li>
                    <li>
                        <b>Labeling noise and Camera parameters Inaccuracies</b> -
                        Web images collected by searching the names of celebrities often contain errors.
                        Our synthetic data has <i>guaranteed correctness of camera parameetrs and labels</i>.
                    </li>
                </ul>
            </div>
        </div>
    </section>    
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                About the Dataset
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    The GeoGen dataset is a comprehensive collection of over 70,000 synthetic face images designed for advanced face recognition and 3D geometry reconstruction research.
                </p>
                <p>
                    The dataset is essential for training deep learning models that are geared towards high-fidelity 3D facial geometry reconstruction.
                </p>
                <p>
                    This dataset was introduced in our paper titled GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions and can be used to train deep learning models for 3D Reconstruction.
                </p>
                <p>
                    The dataset contains:
                </p>
                <ul>
                    <li>
                        The dataset comprises 70,000 images distributed across 10,000 identities, with each identity represented by 7 unique images. These images are captured from various camera angles spanning a full 360-degree view to enhance the diversity and comprehensiveness of the dataset.
                    </li>
                </ul>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    We build on the synthetic face generation framework of <a href="https://microsoft.github.io/FaceSynthetics/">Wood et al.</a> to create a dataset of <b>over one million</b> synthetic face images.
                    This synthetics dataset helps us to overcome three primary shortcomings of existing large-scale face recognition datasets:
                </p>
                <p>
                    We define identity as a unique combination of facial geometry, texture, eye color and hair style.
                    For each identity, we sample a set of accessories including clothing, make-up, glasses, face-wear and head-wear.
                </p>
            </div>
            <div class="columns">
                <div class="column">
                    <img src="img/accessories_id1.jpg" />
                </div>
                <div class="column is-hidden-mobile">
                    <img src="img/accessories_id2.jpg" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    While hair style can change for an individual, most people maintain similar hair style (for both facial and head hair) which makes hair style an important cue for the person's identity.
                    Therefore, for the same identity, we randomize only the color, density and thickness of the hair (top row) and avoiding the impression of changing identity (bottom row).
                    This simulates aging to some extent as hair typically becomes grayer, sparser and thinner during aging.
                    The hair style is only changed when the added head-wear is not compatible with the original hair style.
                </p>
            </div>
            <div class="columns">
                <div class="column" style="padding-right: 0">
                    <img src="img/hair1.jpg" />
                </div>
                <div class="column is-hidden-mobile" style="padding-left: 0">
                    <img src="img/hair2.jpg" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    After sampling the identity and the accessories, we can render multiple images by varying the pose, expression, environment (lighting and background) and camera.
                </p>
            </div>
            <div class="columns">
                <div class="column">
                    <img src="img/variety_id1.jpg" />
                </div>
                <div class="column is-hidden-mobile">
                    <img src="img/variety_id2.jpg" />
                </div>
            </div>
        </div>
    </section>    
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Results
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Qiu_SynFace_Face_Recognition_With_Synthetic_Data_ICCV_2021_paper.html">SynFace</a> is the current state-of-the-art for face recognition model trained on synthetic faces.
                    They used <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Deng_Disentangled_and_Controllable_Face_Image_Generation_via_3D_Imitative-Contrastive_Learning_CVPR_2020_paper.pdf">DiscoFaceGAN</a> to generate 500K synthetic faces of 10K unique identities.
                    We significantly outperform SynFace across all datasets, suggesting that our rendered synthetic faces are better than GAN-generated faces for learning face recognition.
                    This is likely because GAN-generated images do not enforce identity or geometric consistency, and are not effective at changing accessories.
                    The GAN models also have unresolved ethical and bias concerns as they are typically trained on large-scale real face datasets.
                </p>
                <table class="table">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>#images</th>
                            <th class="is-hidden-mobile">LFW</th>
                            <th class="is-hidden-mobile">CFP-FP</th>
                            <th class="is-hidden-mobile">CPLFW</th>
                            <th class="is-hidden-mobile">AgeDB</th>
                            <th class="is-hidden-mobile">CALFW</th>
                            <th>Avg</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><a href="https://openaccess.thecvf.com/content/ICCV2021/html/Qiu_SynFace_Face_Recognition_With_Synthetic_Data_ICCV_2021_paper.html">SynFace</a></td>
                            <td>500K (10K&#215;50)</td>
                            <td class="is-hidden-mobile">91.93</td>
                            <td class="is-hidden-mobile">75.03</td>
                            <td class="is-hidden-mobile">70.43</td>
                            <td class="is-hidden-mobile">61.63</td>
                            <td class="is-hidden-mobile">74.73</td>
                            <td>74.75</td>
                        </tr>
                        <tr>
                            <td>Ours</td>
                            <td>500K (10K&#215;50)</td>
                            <td class="is-hidden-mobile">95.40</td>
                            <td class="is-hidden-mobile">87.40</td>
                            <td class="is-hidden-mobile">78.87</td>
                            <td class="is-hidden-mobile">76.97</td>
                            <td class="is-hidden-mobile">78.62</td>
                            <td>83.45</td>
                        </tr>
                        <tr>
                            <td>Ours</td>
                            <td>1.22M (10K&#215;72&#43;100K&#215;5)</td>
                            <td class="is-hidden-mobile"><b>95.82</b></td>
                            <td class="is-hidden-mobile"><b>88.77</b></td>
                            <td class="is-hidden-mobile"><b>81.62</b></td>
                            <td class="is-hidden-mobile"><b>79.72</b></td>
                            <td class="is-hidden-mobile"><b>80.70</b></td>
                            <td><b>85.32</b></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    When a small number of real face images are available, we can use them to fine-tune the network that is pre-trained on our synthetic data.
                    Such fine-tuning significantly improves the accuracy across all datasets.
                </p>
            </div>
            <div class="columns">
                <div class="column has-text-centered">
                    <img src="img/mix_real_lfw.jpg" />
                </div>
                <div class="column is-hidden-mobile has-text-centered">
                    <img src="img/mix_real_cdp-fp.jpg" />
                </div>
                <div class="column is-hidden-mobile has-text-centered">
                    <img src="img/mix_real_cplfw.jpg" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    However, there remains a substantial accuracy gap from the state-of-the-art methods that are trained on large-scale real face datasets.
                    This gap can be reduced by adopting better data augmentation or by improving the realism of the face generation pipeline.
                    We leave this as future work.
                </p>
                <table class="table">
                    <thead>
                        <tr>
                            <th>Method</th>
                            <th>#synth images</th>
                            <th>#real images</th>
                            <th class="is-hidden-mobile">LFW</th>
                            <th class="is-hidden-mobile">CFP-FP</th>
                            <th class="is-hidden-mobile">CPLFW</th>
                            <th class="is-hidden-mobile">AgeDB</th>
                            <th class="is-hidden-mobile">CALFW</th>
                            <th>Avg</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Ours</td>
                            <td>1.22M</td>
                            <td>0</td>
                            <td class="is-hidden-mobile">96.17</td>
                            <td class="is-hidden-mobile">89.81</td>
                            <td class="is-hidden-mobile">82.23</td>
                            <td class="is-hidden-mobile">81.10</td>
                            <td class="is-hidden-mobile">82.55</td>
                            <td>86.37</td>
                        </tr>
                        <tr>
                            <td>Ours + Real</td>
                            <td>1.22M</td>
                            <td>120K</td>
                            <td class="is-hidden-mobile">99.33</td>
                            <td class="is-hidden-mobile">95.93</td>
                            <td class="is-hidden-mobile">89.47</td>
                            <td class="is-hidden-mobile">91.55</td>
                            <td class="is-hidden-mobile">91.78</td>
                            <td>93.61</td>
                        </tr>
                        <tr>
                            <td><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_CosFace_Large_Margin_CVPR_2018_paper.html">CosFace</a></td>
                            <td>0</td>
                            <td>5.8M</td>
                            <td class="is-hidden-mobile">99.78</td>
                            <td class="is-hidden-mobile">98.26</td>
                            <td class="is-hidden-mobile">92.18</td>
                            <td class="is-hidden-mobile"><b>98.17</b></td>
                            <td class="is-hidden-mobile"><b>96.18</b></td>
                            <td>96.91</td>
                        </tr>
                        <tr>
                            <td><a href="https://openaccess.thecvf.com/content/CVPR2021/html/Meng_MagFace_A_Universal_Representation_for_Face_Recognition_and_Quality_Assessment_CVPR_2021_paper.html">MagFace</a></td>
                            <td>0</td>
                            <td>5.8M</td>
                            <td class="is-hidden-mobile"><b>99.83</b></td>
                            <td class="is-hidden-mobile">98.46</td>
                            <td class="is-hidden-mobile">92.87</td>
                            <td class="is-hidden-mobile"><b>98.17</b></td>
                            <td class="is-hidden-mobile">96.15</td>
                            <td>97.10</td>
                        </tr>
                        <tr>
                            <td><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_AdaFace_Quality_Adaptive_Margin_for_Face_Recognition_CVPR_2022_paper.html">AdaFace</a></td>
                            <td>0</td>
                            <td>5.8M</td>
                            <td class="is-hidden-mobile">99.82</td>
                            <td class="is-hidden-mobile"><b>98.49</b></td>
                            <td class="is-hidden-mobile"><b>93.53</b></td>
                            <td class="is-hidden-mobile">98.05</td>
                            <td class="is-hidden-mobile">96.08</td>
                            <td><b>97.19</b></td>
                        </tr>
                    </tbody>
                </table>
            </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
                @inproceedings{esposito2024geogen,
                    title={GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions},
                    author={Esposito, Salvatore and Xu, Qingshan and Kania, Kacper and Hewitt, Charlie and Mariotti, Octave and Petikam, Lohit and Valentin, Julien and Onken, Arno and Mac Aodha, Oisin},
                    booktitle={Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)},
                    year={2024}
                    organization={IEEE}
                  }
</pre>
        </div>
    </section>
    <footer class="footer pb-0">
        <div class="content has-text-centered pb-5">
            <p>
                Work conducted at <a
                    href=https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge>Mesh Labs &ndash;
                    Cambridge</a>.<br />
                <img src="img/Microsoft-logo-only.svg" class="mt-5" alt="Microsoft" style="height: 2rem;">
            </p>
        </div>
        <div class="footer-links content has-text-centered pt-5 has-text-grey-lighter is-size-7">
            <a href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy</a>
            <a href="https://go.microsoft.com/fwlink/?LinkID=206977">Terms of Use</a>
            <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks">Trademarks</a>
            <a href="https://microsoft.com">&copy; Microsoft 2024</a>
        </div>
    </footer>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>
