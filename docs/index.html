<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>GeoGen</title>
    <link rel="shortcut icon" type="image/jpg" href="img/favicon.ico" />
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <script src="https://kit.fontawesome.com/49f46e7382.js" crossorigin="anonymous"></script>

</head>

<body>
    <nav class="navbar is-dark" role="navigation" aria-label="main navigation">
        <div class="container is-max-desktop">
            <div class="navbar-brand">
                <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                    <img src="img/Microsoft-logo.svg" alt="Mesh Labs – Cambridge" style="height: 1.4rem;">
                </a>
                <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false" data-target="navbarBasicExample">
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                    <span aria-hidden="true"></span>
                </a>
            </div>
            <div id="navbarBasicExample" class="navbar-menu">
                <div class="navbar-start">
                    <a class="navbar-item" href="https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge/">
                        Mesh Labs
                    </a>
                </div>
                <div class="navbar-end">
                    <a class="navbar-item" href="https://cvpr.thecvf.com/virtual/2024">
                        <img class="is-hidden-touch" src="img/cvpr-logo-light.svg" style="height: 100%;" alt="CVPR 2024">
                        <img class="is-hidden-desktop" src="img/cvpr-logo-dark.svg" style="height: 100%;" alt="CVPR 2024">
                    </a>
                </div>
            </div>
        </div>
    </nav>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-2 is-size-3-mobile is-spaced has-text-centered">
                GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions
            </h1>
            <p class="subtitle is-5 has-text-centered has-text-grey">
                Computer Vision and Pattern Recognition 2024<br><a href="https://generative-vision.github.io/workshop-CVPR-24/" class="has-text-grey">2nd Workshop on Generative Models for Computer Vision</a>
            </p>
            <p class="subtitle is-6 has-text-centered authors mt-5" style="line-height: 1.5;">
                <span>
                    <a href="https://iamsalvatore.io/">Salvatore&nbsp;Esposito</a>
                </span>
                <span>
                    <a href="https://ghixu.github.io/">Qingshan&nbsp;Xu</a>
                </span>
                <span>
                    <a href="https://kacper.ai/">Kacper&nbsp;Kania</a>
                </span>
                <span>
                    <a href="https://chewitt.me/">Charlie&nbsp;Hewitt</a>
                </span>
                <span>
                    <a href="mailto:omariott@exseed.ed.ac.uk">Octave&nbsp;Mariotti</a>
                </span>
                <span>
                    <a href="https://lohit.dev/">Lohit&nbsp;Petikam</a>
                </span>
                <span>
                    <a href="https://scholar.google.co.uk/citations?user=pZPD0hMAAAAJ&hl=en">Julien&nbsp;Valentin</a>
                </span>
                <span>
                    <a href="https://homepages.inf.ed.ac.uk/aonken/">Arno&nbsp;Onken</a>
                </span>
                <span>
                    <a href="https://homepages.inf.ed.ac.uk/omacaod/">Oisin&nbsp;Mac&nbsp;Aodha</a>
                </span>
            </p>
        </div>
        <div class="container is-max-desktop has-text-centered mt-5">
            <a href="https://openaccess.thecvf.com/content/CVPR2024W/GCV/html/Esposito_GeoGen_Geometry-Aware_Generative_Modeling_via_Signed_Distance_Functions_CVPRW_2024_paper.html" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
            </a>
            <a href="https://arxiv.org/abs/2406.04254" class="button is-rounded is-link is-light mr-2">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
            </a>
            <a href="https://github.com/microsoft/GeoGen" class="button is-rounded is-link is-light">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Dataset</span>
            </a>
        </div>
    </section>
    <section>
        <div class="container is-max-desktop">
            <figure class="image">
                <img src="img/sx-data.jpg" />
            </figure>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                Abstract
            </h1>
            <div class="content has-text-justified-desktop">
                We introduce a new generative approach for synthesizing 3D geometry and images from single-view collections.
                Most existing approaches predict volumetric density to render multi-view consistent images.
                By employing volumetric rendering using neural radiance fields, they inherit a key limitation: the generated geometry is noisy and unconstrained, limiting the quality and utility of the output meshes.
                To address this issue, we propose GeoGen, a new SDF-based 3D generative model trained in an end-to-end manner.
                Initially, we reinterpret the volumetric density as a Signed Distance Function (SDF).
                This allows us to introduce useful priors to generate valid meshes.
                However, those priors prevent the generative model from learning details, limiting the applicability of the method to real-world scenarios.
                To alleviate that problem, we make the transformation learnable and constrain the rendered depth map to be consistent with the zero-level set of the SDF.
                Through the lens of adversarial training, we encourage the network to produce higher fidelity details on the output meshes.
                For evaluation, we introduce a synthetic dataset of human avatars captured from 360-degree camera angles, to overcome the challenges presented by real-world datasets, which often lack 3D consistency and do not cover all camera angles.
                Our experiments on multiple datasets show that GeoGen produces visually and quantitatively better geometry than the previous generative models based on neural radiance fields.
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                About the Dataset
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    The GeoGen dataset is a comprehensive collection of over 70,000 synthetic face images spanning 360 degrees views designed for 3D geometry reconstruction research.
                    We build on the synthetic face generation framework of <a href="https://microsoft.github.io/FaceSynthetics/">Wood et al.</a>
                </p>
                <p>
                    For our dataset, we randomly generate 7 images of 512&times;512 for each of 10,800 identities, ensuring a comprehensive set of different views, encompassing full azimuthal coverage</p>
                <p>
                    This dataset, introduced in our paper significantly enhances the realism and usability for 3D applications by focusing on the accuracy of camera parameters and the ability to generate easily attainable pseudo ground truths.
                </p>
                <ul>
                    <li>
                        <strong>Accurate Camera Parameters:</strong> GeoGen sets itself apart with highly accurate camera parameters for each image, ensuring reliable input for tasks demanding exact geometric precision.
                    </li>
                    <li>
                        <strong>Full Azimuthal Coverage:</strong> By providing images from a full 360-degree spectrum, the dataset offers unparalleled views of synthetic human heads, crucial for thorough 3D reconstructions.
                    </li>
                    <li>
                        <strong>Pseudo Ground Truths:</strong> The dataset facilitates the generation of pseudo ground truths using advanced multi-view stereo and surface reconstruction techniques, enabling detailed quantitative evaluations of 3D models.
                    </li>
                </ul>
                <p>
                    With its robust framework and detailed captures, the GeoGen dataset can be an essential resource for researchers and developers working on next-generation 3D modeling technologies.
                </p>
            </div>
            <div class="columns">
                <div class="column">
                    <img src="img/accessories_id1.png" />
                </div>
            </div>
            <div class="content has-text-justified-desktop">
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
             Qualitative and Quantitative 3D Reconstruction Results
            </h1>
            <div class="content has-text-justified-desktop">
                <p>
                    Our latest advances in 3D geometry reconstruction, as detailed in our findings for both ShapeNet Cars and Synthetic Heads, demonstrate significant improvements over previous methods.
                    By incorporating Signed Distance Functions (SDF) and Depth Loss, GeoGen achieves superior accuracy and detail in reconstructed models.
                </p>
                <div class="column is-hidden-mobile">
                    <img src="img/accessories_id3.png" />
                </div>
                <div class="column is-hidden-mobile">
                    <img src="img/accessories_id2.png" />
                </div>
                <div class="table-container">
                    <table class="table">
                        <caption>
                            Comparison of different 3D reconstruction metrics for generative models on <em>ShapeNet Cars</em> and our <em>Synthetic Heads</em> dataset.
                            We report averages for MSE, HD, and MSD metrics. Variations of GeoGen without the SDF and Depth Loss constraints are also shown.
                            Best methods for each dataset are bolded.
                        </caption>
                        <thead>
                            <tr>
                                <th>Method</th>
                                <th>Chamfer ↓</th>
                                <th>MSE ↓</th>
                                <th>HD ↓</th>
                                <th>EMD ↓</th>
                                <th>MSD ↓</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <th colspan="6">ShapeNet Cars</th>
                            </tr>
                            <tr>
                                <td>EG3D</td>
                                <td>0.31</td>
                                <td>0.31</td>
                                <td>0.85</td>
                                <td>0.44</td>
                                <td>0.33</td>
                            </tr>
                            <tr>
                                <td>GeoGen w/o SDF&Depth Loss</td>
                                <td>0.27</td>
                                <td>0.28</td>
                                <td><b>0.77</b></td>
                                <td>0.42</td>
                                <td>0.31</td>
                            </tr>
                            <tr>
                                <td><b>GeoGen</b></td>
                                <td><b>0.25</b></td>
                                <td><b>0.27</b></td>
                                <td><b>0.77</b></td>
                                <td><b>0.40</b></td>
                                <td><b>0.29</b></td>
                            </tr>
                            <tr>
                                <th colspan="6">Synthetic Heads</th>
                            </tr>
                            <tr>
                                <td>EG3D</td>
                                <td>0.21</td>
                                <td>0.29</td>
                                <td>0.65</td>
                                <td>0.54</td>
                                <td>0.35</td>
                            </tr>
                            <tr>
                                <td>GeoGen w/o SDF& Depth Loss</td>
                                <td>0.19</td>
                                <td>0.29</td>
                                <td>0.59</td>
                                <td>0.45</td>
                                <td>0.26</td>
                            </tr>
                            <tr>
                                <td><b>GeoGen</b></td>
                                <td><b>0.17</b></td>
                                <td><b>0.27</b></td>
                                <td><b>0.56</b></td>
                                <td><b>0.43</b></td>
                                <td><b>0.24</b></td>
                            </tr>
                        </tbody>
                    </table>
                </div>
                <p>
                    These results highlight our model's capability in providing detailed and accurate reconstructions, reducing metrics like Chamfer and MSE significantly across all tested models, and improving handling metrics like HD, EMD, and MSD.
                </p>
            </div>
            <div class="content has-text-justified-desktop">
                <p>
                    The precision in our 3D models showcases our capability to tackle complex reconstruction challenges.
                    These results are pivotal for applications requiring precise geometric data and serve as a benchmark for future developments in the field.
                </p>
            </div>
        </div>
    </section>
    <section class="section pt-0">
        <div class="container is-max-desktop">
            <h1 class="title is-4">
                BibTeX
            </h1>
            <pre>
@inproceedings{esposito2024geogen,
    author = {Esposito, Salvatore and Xu, Qingshan and Kania, Kacper and Hewitt, Charlie and Mariotti, Octave and Petikam, Lohit and Valentin, Julien and Onken, Arno and Mac Aodha, Oisin},
    title = {GeoGen: Geometry-Aware Generative Modeling via Signed Distance Functions},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month = {June},
    year = {2024},
    pages = {7479-7488}
}
</pre>
        </div>
    </section>
    <footer class="footer pb-0">
        <div class="content has-text-centered pb-5">
            <p>
                Work conducted at <a
                    href=https://www.microsoft.com/en-us/research/lab/mixed-reality-ai-lab-cambridge>Mesh Labs &ndash;
                    Cambridge</a>.<br />
                <img src="img/Microsoft-logo-only.svg" class="mt-5" alt="Microsoft" style="height: 2rem;">
            </p>
        </div>
        <div class="footer-links content has-text-centered pt-5 has-text-grey-lighter is-size-7">
            <a href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy</a>
            <a href="https://go.microsoft.com/fwlink/?LinkID=206977">Terms of Use</a>
            <a href="https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks">Trademarks</a>
            <a href="https://microsoft.com">&copy; Microsoft 2024</a>
        </div>
    </footer>
</body>

<script>
    document.addEventListener('DOMContentLoaded', () => {

        // Get all "navbar-burger" elements
        const $navbarBurgers = Array.prototype.slice.call(document.querySelectorAll('.navbar-burger'), 0);

        // Check if there are any navbar burgers
        if ($navbarBurgers.length > 0) {

            // Add a click event on each of them
            $navbarBurgers.forEach(el => {
                el.addEventListener('click', () => {

                    // Get the target from the "data-target" attribute
                    const target = el.dataset.target;
                    const $target = document.getElementById(target);

                    // Toggle the "is-active" class on both the "navbar-burger" and the "navbar-menu"
                    el.classList.toggle('is-active');
                    $target.classList.toggle('is-active');

                });
            });
        }
    });
</script>

</html>
